{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4085e76-9290-4c64-b167-060214572465",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fc31c78-a331-4e5b-b008-6c497e174fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(888)\n",
    "tensorflow.random.set_seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee80f7fb-b25e-498e-8fe1-a1ab66840005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d89a214-966d-471d-bf46-b097f8e5af72",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5733e270-7867-42da-a420-647b419f62fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH = '11_MNIST/digit_xtrain.csv'\n",
    "X_TEST_PATH = '11_MNIST/digit_xtest.csv'\n",
    "Y_TRAIN_PATH = '11_MNIST/digit_ytrain.csv'\n",
    "Y_TEST_PATH = '11_MNIST/digit_ytest.csv'\n",
    "\n",
    "NR_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17a7c8-9523-4fae-bb68-703a65fcedca",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac4cce25-92da-408b-81ca-bf59f8cadb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.88 ms, sys: 175 Âµs, total: 5.05 ms\n",
      "Wall time: 4.32 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=int)\n",
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cda5ec85-296f-4ecd-89e0-9599b0ff2177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.67 s, sys: 209 ms, total: 1.88 s\n",
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=int)\n",
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399eb086-3b30-4e84-8faa-817ad623513c",
   "metadata": {},
   "source": [
    "# Explore\n",
    "\n",
    "The images we have are 28x28 pixels and in grey scale, this means we just have one value (int between 0 and 255) that indicates how light or dark our pixel is. 0 means totally white, 255 means totally black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e25cbaf8-6aae-4ed6-a9c3-9de319b369a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels for training have the shape (60000,), for test they are (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(f'labels for training have the shape {y_train_all.shape}, for test they are {y_test_all.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64dde09a-fa5c-4b17-8770-808f0201abbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features for training have the shape (60000, 784), for test they are (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(f'features for training have the shape {x_train_all.shape}, for test they are {x_test_all.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0470ad6-5a4a-451a-ac66-6112135c1c24",
   "metadata": {},
   "source": [
    "**Note** for future projects and tutorials (we might not take it into consideration in this one): It's not very good to flatten our features when working with images, having a flat list of the 784 pixels (28x28). Why? Because we lose information of the position of each pixel, we don't know which pixels are surrounding others.\n",
    "\n",
    "Let's take a look at one spare image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "256b8101-0e99-4ac6-9b86-2e45e6d02dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddef4fa-c976-452b-a786-24e11de60f11",
   "metadata": {},
   "source": [
    "And let's now look at the first 5 spare labels (which corresponds to the categories or classes of the first images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "601fe59b-8754-411d-86c7-e51c1201d0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1eaf0-0921-4e3a-aec9-ff39e84cccdd",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Re-scale our features\n",
    "Why? Because our output (target) will be very low, a number between 0 and 9. When this happens, it helps if our input data is also low, e.g. numbers between 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "250351ab-aba3-4188-9ba1-2c27eb1afdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_all, x_test = x_train_all / 255.0, x_test / 255.0 # We're re-scaling + converting to floats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99620e58-dc9e-4f22-b6de-9cc3f27a0a94",
   "metadata": {},
   "source": [
    "## Convert target values into one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bd069-707f-44dd-a2ee-dd9a908f641f",
   "metadata": {},
   "source": [
    "We'll now convert our y_train_all, which is a spare matrix, into a full matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "915adc23-d1d9-44c7-8e0b-624615f68ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = y_train_all[:5]\n",
    "np.eye(10)[values] # see .eye() documentation for understanding, values represent the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cbfe479a-6586-4005-afc1-098c3e0f8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.eye(NR_CLASSES)[y_train_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "142da29d-ea68-4bc1-8ea8-70a24d9a76d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03e3f6f7-1a66-476b-b329-6d52b96aed3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.eye(NR_CLASSES)[y_test]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65c74ef-b4de-411a-a4ba-da4fbaa59c19",
   "metadata": {},
   "source": [
    "## Create validation dataset from our training data\n",
    "\n",
    "**Challenge:** Split the training dataset into a smaller training dataset and a validation dataset for the features and the labels. Create four arrays: `x_val`, `y_val`, `x_train`, and `y_train` from `x_train_all` and `y_train_all`. Use the validation size of 10,000 (defined in the constants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6640680e-81b9-46cb-9fcd-16d02ba16040",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train_all[:VALIDATION_SIZE]\n",
    "y_val = y_train_all[:VALIDATION_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "35060284-a85b-446e-83f2-3766c76aee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "888da547-786b-4e97-894a-d7d59a21aec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "73c2accd-7c73-4c82-b645-d77a008bff07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "tf-bootcamp",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "tf-bootcamp",
   "language": "python",
   "name": "tf-bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
